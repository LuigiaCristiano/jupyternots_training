{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e75fda6",
   "metadata": {},
   "source": [
    "# Hands on session on metadata extraction and editing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e7ed6f",
   "metadata": {},
   "source": [
    "### Edit xml files, convert to dictionary and edit the entries ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654459ce",
   "metadata": {},
   "source": [
    "### 1. Option with xml editors: available in Windows and Linux like Notepad, Notepad++ ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e334780",
   "metadata": {},
   "source": [
    "Note that for data publications, <br> \n",
    "service providers or institutional data managers offers tools like web forms, templates\n",
    "like these 2 examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554cd6f3-0fb8-4a3e-a54f-087d45393dc2",
   "metadata": {},
   "source": [
    "If you are publishing your data with the service providers like DataCite, they offer GUI <br>\n",
    "and a number of packages for the metadata editing <br>\n",
    "`https://github.com/datacite/bolognese`\n",
    "Data Cite has metadata editors as web forms. The validation is included in the metadata editing service"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c579b07-ffdf-4c61-8a82-908d428951b9",
   "metadata": {},
   "source": [
    "https://dhvlab.gwi.uni-muenchen.de/datacite-generator/ as alternative e.g...."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbeeffa-0068-4970-8ebc-bcc92595d654",
   "metadata": {},
   "source": [
    "The FDM colleagues at HZB are working on an interface for the data publication with a web formular <br> \n",
    "and a the possibility to save locally a doc file to modify and send"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5bd924-b8ca-46ce-9fe4-41aa0f9c83bd",
   "metadata": {},
   "source": [
    "similarly to the following: https://dataservices.gfz-potsdam.de/panmetaworks/metaedit/;<br>\n",
    "https://dataservices.gfz-potsdam.de/4dmb/edit/?action=new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a43fc66",
   "metadata": {},
   "source": [
    "#### __The examples here are if you want to change the xml for your data or data descriptors for updating the metadata__ ####"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6480c06",
   "metadata": {},
   "source": [
    "How to __edit__ from command line an __xml__ file for data publication\n",
    "How to manipulate the metadata entries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0b4ec8",
   "metadata": {},
   "source": [
    "### 2. Command line option using the `Python` environment ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83996b9",
   "metadata": {},
   "source": [
    "For semplicity we download the DataCite schema from the project web page"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8d98f5",
   "metadata": {},
   "source": [
    "We select the metadata structure suitable for dataset publication"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fb1a77",
   "metadata": {},
   "source": [
    "%https://schema.datacite.org/meta/kernel-4.4/\n",
    "https://schema.datacite.org/meta/kernel-4.4/example/datacite-example-dataset-v4.xml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6cb874",
   "metadata": {},
   "source": [
    "We save the xml locally and open it using a Python routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c3a07a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xml.etree.ElementTree.ElementTree object at 0x7f80c442ed90>\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "filename='/home/luigia/jupiter_test/datacite-example-dataset-v4.xml'\n",
    "tree = ET.parse(filename)\n",
    "print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc40a05",
   "metadata": {},
   "source": [
    "the top structure is printed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a373c35",
   "metadata": {},
   "source": [
    "now we visualise the whole hierarchical structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7db655ac",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ns0:resource xmlns:ns0=\"http://datacite.org/schema/kernel-4\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://datacite.org/schema/kernel-4 https://schema.datacite.org/meta/kernel-4.4/metadata.xsd\">\n",
      "  <ns0:identifier identifierType=\"DOI\">10.5072/D3P26Q35R-Test</ns0:identifier>\n",
      "  <ns0:creators>\n",
      "    <ns0:creator>\n",
      "      <ns0:creatorName nameType=\"Personal\">Fosmire, Michael</ns0:creatorName>\n",
      "      <ns0:givenName>Michael</ns0:givenName>\n",
      "      <ns0:familyName>Fosmire</ns0:familyName>\n",
      "    </ns0:creator>\n",
      "    <ns0:creator>\n",
      "      <ns0:creatorName nameType=\"Personal\">Wertz, Ruth</ns0:creatorName>\n",
      "      <ns0:givenName>Ruth</ns0:givenName>\n",
      "      <ns0:familyName>Wertz</ns0:familyName>\n",
      "    </ns0:creator>\n",
      "    <ns0:creator>\n",
      "      <ns0:creatorName nameType=\"Personal\">Purzer, Senay</ns0:creatorName>\n",
      "      <ns0:givenName>Senay</ns0:givenName>\n",
      "      <ns0:familyName>Purzer</ns0:familyName>\n",
      "    </ns0:creator>\n",
      "  </ns0:creators>\n",
      "  <ns0:titles>\n",
      "    <ns0:title xml:lang=\"en\">Critical Engineering Literacy Test (CELT)</ns0:title>\n",
      "  </ns0:titles>\n",
      "  <ns0:publisher xml:lang=\"en\">Purdue University Research Repository (PURR)</ns0:publisher>\n",
      "  <ns0:publicationYear>2013</ns0:publicationYear>\n",
      "  <ns0:subjects>\n",
      "    <ns0:subject xml:lang=\"en\">Assessment</ns0:subject>\n",
      "    <ns0:subject xml:lang=\"en\">Information Literacy</ns0:subject>\n",
      "    <ns0:subject xml:lang=\"en\">Engineering</ns0:subject>\n",
      "    <ns0:subject xml:lang=\"en\">Undergraduate Students</ns0:subject>\n",
      "    <ns0:subject xml:lang=\"en\">CELT</ns0:subject>\n",
      "    <ns0:subject xml:lang=\"en\">Purdue University</ns0:subject>\n",
      "  </ns0:subjects>\n",
      "  <ns0:language>en</ns0:language>\n",
      "  <ns0:resourceType resourceTypeGeneral=\"Dataset\">Dataset</ns0:resourceType>\n",
      "  <ns0:version>1.0</ns0:version>\n",
      "  <ns0:descriptions>\n",
      "    <ns0:description xml:lang=\"en\" descriptionType=\"Abstract\">We developed an instrument, Critical Engineering Literacy Test (CELT), which is a multiple choice instrument designed to measure undergraduate students’ scientific and information literacy skills. It requires students to first read a technical memo and, based on the memo’s arguments, answer eight multiple choice and six open-ended response questions. We collected data from 143 first-year engineering students and conducted an item analysis. The KR-20 reliability of the instrument was .39. Item difficulties ranged between .17 to .83. The results indicate low reliability index but acceptable levels of item difficulties and item discrimination indices. Students were most challenged when answering items measuring scientific and mathematical literacy (i.e., identifying incorrect information).</ns0:description>\n",
      "  </ns0:descriptions>\n",
      "</ns0:resource>\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(ET.dump(tree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91743e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Element '{http://datacite.org/schema/kernel-4}resource' at 0x7f6d602bc0e0>\n"
     ]
    }
   ],
   "source": [
    "root = tree.getroot()\n",
    "print(root)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba176d42",
   "metadata": {},
   "source": [
    "We can visualize the tag for each element and the corresponding entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "89e886f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{http://datacite.org/schema/kernel-4}resource'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root.tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2482ec5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'{http://www.w3.org/2001/XMLSchema-instance}schemaLocation': 'http://datacite.org/schema/kernel-4 https://schema.datacite.org/meta/kernel-4.4/metadata.xsd'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root.attrib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d862422c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{http://datacite.org/schema/kernel-4}identifier {'identifierType': 'DOI'}\n",
      "{http://datacite.org/schema/kernel-4}creators {}\n",
      "{http://datacite.org/schema/kernel-4}titles {}\n",
      "{http://datacite.org/schema/kernel-4}publisher {'{http://www.w3.org/XML/1998/namespace}lang': 'en'}\n",
      "{http://datacite.org/schema/kernel-4}publicationYear {}\n",
      "{http://datacite.org/schema/kernel-4}subjects {}\n",
      "{http://datacite.org/schema/kernel-4}language {}\n",
      "{http://datacite.org/schema/kernel-4}resourceType {'resourceTypeGeneral': 'Dataset'}\n",
      "{http://datacite.org/schema/kernel-4}version {}\n",
      "{http://datacite.org/schema/kernel-4}descriptions {}\n"
     ]
    }
   ],
   "source": [
    "for child in root:\n",
    "    print(child.tag,child.attrib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "88f540e0",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{http://datacite.org/schema/kernel-4}creator {}\n",
      "{http://datacite.org/schema/kernel-4}creator {}\n",
      "{http://datacite.org/schema/kernel-4}creator {}\n",
      "{http://datacite.org/schema/kernel-4}title {'{http://www.w3.org/XML/1998/namespace}lang': 'en'}\n",
      "{http://datacite.org/schema/kernel-4}subject {'{http://www.w3.org/XML/1998/namespace}lang': 'en'}\n",
      "{http://datacite.org/schema/kernel-4}subject {'{http://www.w3.org/XML/1998/namespace}lang': 'en'}\n",
      "{http://datacite.org/schema/kernel-4}subject {'{http://www.w3.org/XML/1998/namespace}lang': 'en'}\n",
      "{http://datacite.org/schema/kernel-4}subject {'{http://www.w3.org/XML/1998/namespace}lang': 'en'}\n",
      "{http://datacite.org/schema/kernel-4}subject {'{http://www.w3.org/XML/1998/namespace}lang': 'en'}\n",
      "{http://datacite.org/schema/kernel-4}subject {'{http://www.w3.org/XML/1998/namespace}lang': 'en'}\n",
      "{http://datacite.org/schema/kernel-4}description {'{http://www.w3.org/XML/1998/namespace}lang': 'en', 'descriptionType': 'Abstract'}\n"
     ]
    }
   ],
   "source": [
    "for child in root:\n",
    "    for subchild in child:\n",
    "        print(subchild.tag,subchild.attrib)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c186c6",
   "metadata": {},
   "source": [
    "Loop over all the xml elements and build a list of keys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29696c11",
   "metadata": {},
   "source": [
    "#### Can be convenient convert the  xml structure to a dictionary ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "191222cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"resource\": {\n",
      "        \"@xmlns:xsi\": \"http://www.w3.org/2001/XMLSchema-instance\",\n",
      "        \"@xmlns\": \"http://datacite.org/schema/kernel-4\",\n",
      "        \"@xsi:schemaLocation\": \"http://datacite.org/schema/kernel-4 https://schema.datacite.org/meta/kernel-4.4/metadata.xsd\",\n",
      "        \"identifier\": {\n",
      "            \"@identifierType\": \"DOI\",\n",
      "            \"#text\": \"10.5072/D3P26Q35R-Test\"\n",
      "        },\n",
      "        \"creators\": {\n",
      "            \"creator\": [\n",
      "                {\n",
      "                    \"creatorName\": {\n",
      "                        \"@nameType\": \"Personal\",\n",
      "                        \"#text\": \"Fosmire, Michael\"\n",
      "                    },\n",
      "                    \"givenName\": \"Michael\",\n",
      "                    \"familyName\": \"Fosmire\"\n",
      "                },\n",
      "                {\n",
      "                    \"creatorName\": {\n",
      "                        \"@nameType\": \"Personal\",\n",
      "                        \"#text\": \"Wertz, Ruth\"\n",
      "                    },\n",
      "                    \"givenName\": \"Ruth\",\n",
      "                    \"familyName\": \"Wertz\"\n",
      "                },\n",
      "                {\n",
      "                    \"creatorName\": {\n",
      "                        \"@nameType\": \"Personal\",\n",
      "                        \"#text\": \"Purzer, Senay\"\n",
      "                    },\n",
      "                    \"givenName\": \"Senay\",\n",
      "                    \"familyName\": \"Purzer\"\n",
      "                }\n",
      "            ]\n",
      "        },\n",
      "        \"titles\": {\n",
      "            \"title\": {\n",
      "                \"@xml:lang\": \"en\",\n",
      "                \"#text\": \"Critical Engineering Literacy Test (CELT)\"\n",
      "            }\n",
      "        },\n",
      "        \"publisher\": {\n",
      "            \"@xml:lang\": \"en\",\n",
      "            \"#text\": \"Purdue University Research Repository (PURR)\"\n",
      "        },\n",
      "        \"publicationYear\": \"2013\",\n",
      "        \"subjects\": {\n",
      "            \"subject\": [\n",
      "                {\n",
      "                    \"@xml:lang\": \"en\",\n",
      "                    \"#text\": \"Assessment\"\n",
      "                },\n",
      "                {\n",
      "                    \"@xml:lang\": \"en\",\n",
      "                    \"#text\": \"Information Literacy\"\n",
      "                },\n",
      "                {\n",
      "                    \"@xml:lang\": \"en\",\n",
      "                    \"#text\": \"Engineering\"\n",
      "                },\n",
      "                {\n",
      "                    \"@xml:lang\": \"en\",\n",
      "                    \"#text\": \"Undergraduate Students\"\n",
      "                },\n",
      "                {\n",
      "                    \"@xml:lang\": \"en\",\n",
      "                    \"#text\": \"CELT\"\n",
      "                },\n",
      "                {\n",
      "                    \"@xml:lang\": \"en\",\n",
      "                    \"#text\": \"Purdue University\"\n",
      "                }\n",
      "            ]\n",
      "        },\n",
      "        \"language\": \"en\",\n",
      "        \"resourceType\": {\n",
      "            \"@resourceTypeGeneral\": \"Dataset\",\n",
      "            \"#text\": \"Dataset\"\n",
      "        },\n",
      "        \"version\": \"1.0\",\n",
      "        \"descriptions\": {\n",
      "            \"description\": {\n",
      "                \"@xml:lang\": \"en\",\n",
      "                \"@descriptionType\": \"Abstract\",\n",
      "                \"#text\": \"We developed an instrument, Critical Engineering Literacy Test (CELT), which is a multiple choice instrument designed to measure undergraduate students\\u2019 scientific and information literacy skills. It requires students to first read a technical memo and, based on the memo\\u2019s arguments, answer eight multiple choice and six open-ended response questions. We collected data from 143 first-year engineering students and conducted an item analysis. The KR-20 reliability of the instrument was .39. Item difficulties ranged between .17 to .83. The results indicate low reliability index but acceptable levels of item difficulties and item discrimination indices. Students were most challenged when answering items measuring scientific and mathematical literacy (i.e., identifying incorrect information).\"\n",
      "            }\n",
      "        }\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import xmltodict\n",
    "import json\n",
    "with open(filename) as fn:\n",
    "    doc=xmltodict.parse(fn.read())\n",
    "print(json.dumps(doc, indent=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56d62c01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('resource', OrderedDict([('@xmlns:xsi', 'http://www.w3.org/2001/XMLSchema-instance'), ('@xmlns', 'http://datacite.org/schema/kernel-4'), ('@xsi:schemaLocation', 'http://datacite.org/schema/kernel-4 https://schema.datacite.org/meta/kernel-4.4/metadata.xsd'), ('identifier', OrderedDict([('@identifierType', 'DOI'), ('#text', '10.5072/D3P26Q35R-Test')])), ('creators', OrderedDict([('creator', [OrderedDict([('creatorName', OrderedDict([('@nameType', 'Personal'), ('#text', 'Fosmire, Michael')])), ('givenName', 'Michael'), ('familyName', 'Fosmire')]), OrderedDict([('creatorName', OrderedDict([('@nameType', 'Personal'), ('#text', 'Wertz, Ruth')])), ('givenName', 'Ruth'), ('familyName', 'Wertz')]), OrderedDict([('creatorName', OrderedDict([('@nameType', 'Personal'), ('#text', 'Purzer, Senay')])), ('givenName', 'Senay'), ('familyName', 'Purzer')])])])), ('titles', OrderedDict([('title', OrderedDict([('@xml:lang', 'en'), ('#text', 'Critical Engineering Literacy Test (CELT)')]))])), ('publisher', OrderedDict([('@xml:lang', 'en'), ('#text', 'Purdue University Research Repository (PURR)')])), ('publicationYear', '2013'), ('subjects', OrderedDict([('subject', [OrderedDict([('@xml:lang', 'en'), ('#text', 'Assessment')]), OrderedDict([('@xml:lang', 'en'), ('#text', 'Information Literacy')]), OrderedDict([('@xml:lang', 'en'), ('#text', 'Engineering')]), OrderedDict([('@xml:lang', 'en'), ('#text', 'Undergraduate Students')]), OrderedDict([('@xml:lang', 'en'), ('#text', 'CELT')]), OrderedDict([('@xml:lang', 'en'), ('#text', 'Purdue University')])])])), ('language', 'en'), ('resourceType', OrderedDict([('@resourceTypeGeneral', 'Dataset'), ('#text', 'Dataset')])), ('version', '1.0'), ('descriptions', OrderedDict([('description', OrderedDict([('@xml:lang', 'en'), ('@descriptionType', 'Abstract'), ('#text', 'We developed an instrument, Critical Engineering Literacy Test (CELT), which is a multiple choice instrument designed to measure undergraduate students’ scientific and information literacy skills. It requires students to first read a technical memo and, based on the memo’s arguments, answer eight multiple choice and six open-ended response questions. We collected data from 143 first-year engineering students and conducted an item analysis. The KR-20 reliability of the instrument was .39. Item difficulties ranged between .17 to .83. The results indicate low reliability index but acceptable levels of item difficulties and item discrimination indices. Students were most challenged when answering items measuring scientific and mathematical literacy (i.e., identifying incorrect information).')]))]))]))])\n"
     ]
    }
   ],
   "source": [
    "print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "659a46cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=json.dumps(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b0980f0d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"resource\": {\"@xmlns:xsi\": \"http://www.w3.org/2001/XMLSchema-instance\", \"@xmlns\": \"http://datacite.org/schema/kernel-4\", \"@xsi:schemaLocation\": \"http://datacite.org/schema/kernel-4 https://schema.datacite.org/meta/kernel-4.4/metadata.xsd\", \"identifier\": {\"@identifierType\": \"DOI\", \"#text\": \"10.5072/D3P26Q35R-Test\"}, \"creators\": {\"creator\": [{\"creatorName\": {\"@nameType\": \"Personal\", \"#text\": \"Fosmire, Michael\"}, \"givenName\": \"Michael\", \"familyName\": \"Fosmire\"}, {\"creatorName\": {\"@nameType\": \"Personal\", \"#text\": \"Wertz, Ruth\"}, \"givenName\": \"Ruth\", \"familyName\": \"Wertz\"}, {\"creatorName\": {\"@nameType\": \"Personal\", \"#text\": \"Purzer, Senay\"}, \"givenName\": \"Senay\", \"familyName\": \"Purzer\"}]}, \"titles\": {\"title\": {\"@xml:lang\": \"en\", \"#text\": \"Critical Engineering Literacy Test (CELT)\"}}, \"publisher\": {\"@xml:lang\": \"en\", \"#text\": \"Purdue University Research Repository (PURR)\"}, \"publicationYear\": \"2013\", \"subjects\": {\"subject\": [{\"@xml:lang\": \"en\", \"#text\": \"Assessment\"}, {\"@xml:lang\": \"en\", \"#text\": \"Information Literacy\"}, {\"@xml:lang\": \"en\", \"#text\": \"Engineering\"}, {\"@xml:lang\": \"en\", \"#text\": \"Undergraduate Students\"}, {\"@xml:lang\": \"en\", \"#text\": \"CELT\"}, {\"@xml:lang\": \"en\", \"#text\": \"Purdue University\"}]}, \"language\": \"en\", \"resourceType\": {\"@resourceTypeGeneral\": \"Dataset\", \"#text\": \"Dataset\"}, \"version\": \"1.0\", \"descriptions\": {\"description\": {\"@xml:lang\": \"en\", \"@descriptionType\": \"Abstract\", \"#text\": \"We developed an instrument, Critical Engineering Literacy Test (CELT), which is a multiple choice instrument designed to measure undergraduate students\\u2019 scientific and information literacy skills. It requires students to first read a technical memo and, based on the memo\\u2019s arguments, answer eight multiple choice and six open-ended response questions. We collected data from 143 first-year engineering students and conducted an item analysis. The KR-20 reliability of the instrument was .39. Item difficulties ranged between .17 to .83. The results indicate low reliability index but acceptable levels of item difficulties and item discrimination indices. Students were most challenged when answering items measuring scientific and mathematical literacy (i.e., identifying incorrect information).\"}}}}\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453231d4",
   "metadata": {},
   "source": [
    "We can visualize the keys of the metadata structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66b66526",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['@xmlns:xsi', '@xmlns', '@xsi:schemaLocation', 'identifier', 'creators', 'titles', 'publisher', 'publicationYear', 'subjects', 'language', 'resourceType', 'version', 'descriptions'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc['resource'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5369eb8c",
   "metadata": {},
   "source": [
    "access specific elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7b0c246f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('description',\n",
       "              OrderedDict([('@xml:lang', 'en'),\n",
       "                           ('@descriptionType', 'Abstract'),\n",
       "                           ('#text',\n",
       "                            'We developed an instrument, Critical Engineering Literacy Test (CELT), which is a multiple choice instrument designed to measure undergraduate students’ scientific and information literacy skills. It requires students to first read a technical memo and, based on the memo’s arguments, answer eight multiple choice and six open-ended response questions. We collected data from 143 first-year engineering students and conducted an item analysis. The KR-20 reliability of the instrument was .39. Item difficulties ranged between .17 to .83. The results indicate low reliability index but acceptable levels of item difficulties and item discrimination indices. Students were most challenged when answering items measuring scientific and mathematical literacy (i.e., identifying incorrect information).')]))])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc['resource']['descriptions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ec0e7cbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['description'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc['resource']['descriptions'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a9a8b99e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We developed an instrument, Critical Engineering Literacy Test (CELT), which is a multiple choice instrument designed to measure undergraduate students’ scientific and information literacy skills. It requires students to first read a technical memo and, based on the memo’s arguments, answer eight multiple choice and six open-ended response questions. We collected data from 143 first-year engineering students and conducted an item analysis. The KR-20 reliability of the instrument was .39. Item difficulties ranged between .17 to .83. The results indicate low reliability index but acceptable levels of item difficulties and item discrimination indices. Students were most challenged when answering items measuring scientific and mathematical literacy (i.e., identifying incorrect information).'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc['resource']['descriptions']['description']['#text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43537e9",
   "metadata": {},
   "source": [
    "We can change the text or any other element and reassign it to the json dictionary and save it as xml for uploading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "18653383",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc['resource']['descriptions']['description']['#text']=\"None\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "597639d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'None'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc['resource']['descriptions']['description']['#text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5dc7a1",
   "metadata": {},
   "source": [
    "We can modify an xml file save it and <br>\n",
    "check if the structure is still valid respect the reference model: <br>\n",
    "the corresponding .xsd file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64515c91",
   "metadata": {},
   "source": [
    "schema validation using the standard schema .xsd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7784f011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warning: failed to load external entity \"include/datacite-numberType-v4.xsd\"\n",
      "metadata_4_4m.xsd:27: element include: Schemas parser error : Element '{http://www.w3.org/2001/XMLSchema}include': Failed to load the document 'include/datacite-numberType-v4.xsd' for inclusion.\n",
      "WXS schema metadata_4_4m.xsd failed to compile\n"
     ]
    }
   ],
   "source": [
    "!xmllint --schema metadata_4_4m.xsd datacite-example-dataset-v4.xml --noout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14fb536",
   "metadata": {},
   "source": [
    "github.com/inveniosoftware/datacite\n",
    "with JSON to schema converters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4115a416",
   "metadata": {},
   "source": [
    "xml structures can be converted to JSON, to csv (keep in mind is strongly nested, it is a difficult) or BiBTeX files"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
