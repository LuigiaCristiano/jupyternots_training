{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cd9ce9f",
   "metadata": {},
   "source": [
    "# Hands on session on metadata extraction and editing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338ee8bf",
   "metadata": {},
   "source": [
    "Conversion steps for uploading metadata and further use of the metadata entries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fb4412",
   "metadata": {},
   "source": [
    "1. download the DataCite schema from Project web page"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945c6f03",
   "metadata": {},
   "source": [
    "We select the metadata structure suitable for dataset publication from DataCite website"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8761c290",
   "metadata": {},
   "source": [
    "https://schema.datacite.org/meta/kernel-4.3/metadata.xsd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311a94fe",
   "metadata": {},
   "source": [
    "We save the xml locally and open it using a Python routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fdf20e1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xml.etree.ElementTree.ElementTree object at 0x7fd4301f4780>\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "filename='datacite-example-full-v4.xml'\n",
    "tree = ET.parse(filename)\n",
    "print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "58e376b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Element '{http://datacite.org/schema/kernel-4}resource' at 0x7fd4301fdcc8>\n"
     ]
    }
   ],
   "source": [
    "root = tree.getroot()\n",
    "print(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fd7bfe17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'{http://www.w3.org/2001/XMLSchema-instance}schemaLocation': 'http://datacite.org/schema/kernel-4 https://schema.datacite.org/meta/kernel-4.4/metadata.xsd'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root.attrib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f38a3992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{http://datacite.org/schema/kernel-4}identifier {'identifierType': 'DOI'}\n",
      "{http://datacite.org/schema/kernel-4}creators {}\n",
      "{http://datacite.org/schema/kernel-4}titles {}\n",
      "{http://datacite.org/schema/kernel-4}publisher {'{http://www.w3.org/XML/1998/namespace}lang': 'en'}\n",
      "{http://datacite.org/schema/kernel-4}publicationYear {}\n",
      "{http://datacite.org/schema/kernel-4}subjects {}\n",
      "{http://datacite.org/schema/kernel-4}language {}\n",
      "{http://datacite.org/schema/kernel-4}resourceType {'resourceTypeGeneral': 'Dataset'}\n",
      "{http://datacite.org/schema/kernel-4}version {}\n",
      "{http://datacite.org/schema/kernel-4}descriptions {}\n"
     ]
    }
   ],
   "source": [
    "for child in root:\n",
    "    print(child.tag,child.attrib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0afdd961",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{http://datacite.org/schema/kernel-4}creator {}\n",
      "{http://datacite.org/schema/kernel-4}creator {}\n",
      "{http://datacite.org/schema/kernel-4}creator {}\n",
      "{http://datacite.org/schema/kernel-4}title {'{http://www.w3.org/XML/1998/namespace}lang': 'en'}\n",
      "{http://datacite.org/schema/kernel-4}subject {'{http://www.w3.org/XML/1998/namespace}lang': 'en'}\n",
      "{http://datacite.org/schema/kernel-4}subject {'{http://www.w3.org/XML/1998/namespace}lang': 'en'}\n",
      "{http://datacite.org/schema/kernel-4}subject {'{http://www.w3.org/XML/1998/namespace}lang': 'en'}\n",
      "{http://datacite.org/schema/kernel-4}subject {'{http://www.w3.org/XML/1998/namespace}lang': 'en'}\n",
      "{http://datacite.org/schema/kernel-4}subject {'{http://www.w3.org/XML/1998/namespace}lang': 'en'}\n",
      "{http://datacite.org/schema/kernel-4}subject {'{http://www.w3.org/XML/1998/namespace}lang': 'en'}\n",
      "{http://datacite.org/schema/kernel-4}description {'{http://www.w3.org/XML/1998/namespace}lang': 'en', 'descriptionType': 'Abstract'}\n"
     ]
    }
   ],
   "source": [
    "for child in root:\n",
    "    for subchild in child:\n",
    "        print(subchild.tag,subchild.attrib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "34d5de21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root.findall('{http://datacite.org/schema/kernel-4}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63fb8e1",
   "metadata": {},
   "source": [
    "Loop over all the xml elements and build a list of keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26dd295a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list=[]\n",
    "for item in root.find('resource'):\n",
    "    data={}\n",
    "    for child i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d45fc99c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xml.etree.ElementTree.XMLPullParser object at 0x7fd4300fcfd0>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser=ET.XMLPullParser(['start','end'])\n",
    "print(parser)\n",
    "list(parser.read_events())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc638d3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2911a2cd",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-10-bee9cbb0ca8f>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-10-bee9cbb0ca8f>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    Print Etree.text, Etree.value, Etree.getchildren()\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Content = open(filename).read()\n",
    "from xml.etree import XML\n",
    "Etree = XML(Content)\n",
    "Print Etree.text, Etree.value, Etree.getchildren()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcec487",
   "metadata": {},
   "outputs": [],
   "source": [
    "Not all parser work optimal on your xml structure, some manual changes might be needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acf0380",
   "metadata": {},
   "source": [
    "Transforming an xml structure to a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5eef2359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"resource\": {\n",
      "        \"@xmlns:xsi\": \"http://www.w3.org/2001/XMLSchema-instance\",\n",
      "        \"@xmlns\": \"http://datacite.org/schema/kernel-4\",\n",
      "        \"@xsi:schemaLocation\": \"http://datacite.org/schema/kernel-4 https://schema.datacite.org/meta/kernel-4.4/metadata.xsd\",\n",
      "        \"identifier\": {\n",
      "            \"@identifierType\": \"DOI\",\n",
      "            \"#text\": \"10.5072/D3P26Q35R-Test\"\n",
      "        },\n",
      "        \"creators\": {\n",
      "            \"creator\": [\n",
      "                {\n",
      "                    \"creatorName\": {\n",
      "                        \"@nameType\": \"Personal\",\n",
      "                        \"#text\": \"Fosmire, Michael\"\n",
      "                    },\n",
      "                    \"givenName\": \"Michael\",\n",
      "                    \"familyName\": \"Fosmire\"\n",
      "                },\n",
      "                {\n",
      "                    \"creatorName\": {\n",
      "                        \"@nameType\": \"Personal\",\n",
      "                        \"#text\": \"Wertz, Ruth\"\n",
      "                    },\n",
      "                    \"givenName\": \"Ruth\",\n",
      "                    \"familyName\": \"Wertz\"\n",
      "                },\n",
      "                {\n",
      "                    \"creatorName\": {\n",
      "                        \"@nameType\": \"Personal\",\n",
      "                        \"#text\": \"Purzer, Senay\"\n",
      "                    },\n",
      "                    \"givenName\": \"Senay\",\n",
      "                    \"familyName\": \"Purzer\"\n",
      "                }\n",
      "            ]\n",
      "        },\n",
      "        \"titles\": {\n",
      "            \"title\": {\n",
      "                \"@xml:lang\": \"en\",\n",
      "                \"#text\": \"Critical Engineering Literacy Test (CELT)\"\n",
      "            }\n",
      "        },\n",
      "        \"publisher\": {\n",
      "            \"@xml:lang\": \"en\",\n",
      "            \"#text\": \"Purdue University Research Repository (PURR)\"\n",
      "        },\n",
      "        \"publicationYear\": \"2013\",\n",
      "        \"subjects\": {\n",
      "            \"subject\": [\n",
      "                {\n",
      "                    \"@xml:lang\": \"en\",\n",
      "                    \"#text\": \"Assessment\"\n",
      "                },\n",
      "                {\n",
      "                    \"@xml:lang\": \"en\",\n",
      "                    \"#text\": \"Information Literacy\"\n",
      "                },\n",
      "                {\n",
      "                    \"@xml:lang\": \"en\",\n",
      "                    \"#text\": \"Engineering\"\n",
      "                },\n",
      "                {\n",
      "                    \"@xml:lang\": \"en\",\n",
      "                    \"#text\": \"Undergraduate Students\"\n",
      "                },\n",
      "                {\n",
      "                    \"@xml:lang\": \"en\",\n",
      "                    \"#text\": \"CELT\"\n",
      "                },\n",
      "                {\n",
      "                    \"@xml:lang\": \"en\",\n",
      "                    \"#text\": \"Purdue University\"\n",
      "                }\n",
      "            ]\n",
      "        },\n",
      "        \"language\": \"en\",\n",
      "        \"resourceType\": {\n",
      "            \"@resourceTypeGeneral\": \"Dataset\",\n",
      "            \"#text\": \"Dataset\"\n",
      "        },\n",
      "        \"version\": \"1.0\",\n",
      "        \"descriptions\": {\n",
      "            \"description\": {\n",
      "                \"@xml:lang\": \"en\",\n",
      "                \"@descriptionType\": \"Abstract\",\n",
      "                \"#text\": \"We developed an instrument, Critical Engineering Literacy Test (CELT), which is a multiple choice instrument designed to measure undergraduate students\\u2019 scientific and information literacy skills. It requires students to first read a technical memo and, based on the memo\\u2019s arguments, answer eight multiple choice and six open-ended response questions. We collected data from 143 first-year engineering students and conducted an item analysis. The KR-20 reliability of the instrument was .39. Item difficulties ranged between .17 to .83. The results indicate low reliability index but acceptable levels of item difficulties and item discrimination indices. Students were most challenged when answering items measuring scientific and mathematical literacy (i.e., identifying incorrect information).\"\n",
      "            }\n",
      "        }\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import xmltodict\n",
    "import json\n",
    "with open(filename) as fn:\n",
    "    doc=xmltodict.parse(fn.read())\n",
    "print(json.dumps(doc, indent=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ad14a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "70035365",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=json.dumps(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d5216879",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"resource\": {\"@xmlns:xsi\": \"http://www.w3.org/2001/XMLSchema-instance\", \"@xmlns\": \"http://datacite.org/schema/kernel-4\", \"@xsi:schemaLocation\": \"http://datacite.org/schema/kernel-4 https://schema.datacite.org/meta/kernel-4.4/metadata.xsd\", \"identifier\": {\"@identifierType\": \"DOI\", \"#text\": \"10.5072/D3P26Q35R-Test\"}, \"creators\": {\"creator\": [{\"creatorName\": {\"@nameType\": \"Personal\", \"#text\": \"Fosmire, Michael\"}, \"givenName\": \"Michael\", \"familyName\": \"Fosmire\"}, {\"creatorName\": {\"@nameType\": \"Personal\", \"#text\": \"Wertz, Ruth\"}, \"givenName\": \"Ruth\", \"familyName\": \"Wertz\"}, {\"creatorName\": {\"@nameType\": \"Personal\", \"#text\": \"Purzer, Senay\"}, \"givenName\": \"Senay\", \"familyName\": \"Purzer\"}]}, \"titles\": {\"title\": {\"@xml:lang\": \"en\", \"#text\": \"Critical Engineering Literacy Test (CELT)\"}}, \"publisher\": {\"@xml:lang\": \"en\", \"#text\": \"Purdue University Research Repository (PURR)\"}, \"publicationYear\": \"2013\", \"subjects\": {\"subject\": [{\"@xml:lang\": \"en\", \"#text\": \"Assessment\"}, {\"@xml:lang\": \"en\", \"#text\": \"Information Literacy\"}, {\"@xml:lang\": \"en\", \"#text\": \"Engineering\"}, {\"@xml:lang\": \"en\", \"#text\": \"Undergraduate Students\"}, {\"@xml:lang\": \"en\", \"#text\": \"CELT\"}, {\"@xml:lang\": \"en\", \"#text\": \"Purdue University\"}]}, \"language\": \"en\", \"resourceType\": {\"@resourceTypeGeneral\": \"Dataset\", \"#text\": \"Dataset\"}, \"version\": \"1.0\", \"descriptions\": {\"description\": {\"@xml:lang\": \"en\", \"@descriptionType\": \"Abstract\", \"#text\": \"We developed an instrument, Critical Engineering Literacy Test (CELT), which is a multiple choice instrument designed to measure undergraduate students\\u2019 scientific and information literacy skills. It requires students to first read a technical memo and, based on the memo\\u2019s arguments, answer eight multiple choice and six open-ended response questions. We collected data from 143 first-year engineering students and conducted an item analysis. The KR-20 reliability of the instrument was .39. Item difficulties ranged between .17 to .83. The results indicate low reliability index but acceptable levels of item difficulties and item discrimination indices. Students were most challenged when answering items measuring scientific and mathematical literacy (i.e., identifying incorrect information).\"}}}}\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "61959ba0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['@xmlns:xsi', '@xmlns', '@xsi:schemaLocation', 'identifier', 'creators', 'titles', 'publisher', 'publicationYear', 'subjects', 'language', 'resourceType', 'version', 'descriptions'])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc['resource'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6f66e25f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('description',\n",
       "              OrderedDict([('@xml:lang', 'en'),\n",
       "                           ('@descriptionType', 'Abstract'),\n",
       "                           ('#text',\n",
       "                            'We developed an instrument, Critical Engineering Literacy Test (CELT), which is a multiple choice instrument designed to measure undergraduate students’ scientific and information literacy skills. It requires students to first read a technical memo and, based on the memo’s arguments, answer eight multiple choice and six open-ended response questions. We collected data from 143 first-year engineering students and conducted an item analysis. The KR-20 reliability of the instrument was .39. Item difficulties ranged between .17 to .83. The results indicate low reliability index but acceptable levels of item difficulties and item discrimination indices. Students were most challenged when answering items measuring scientific and mathematical literacy (i.e., identifying incorrect information).')]))])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc['resource']['descriptions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7a7feda7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['description'])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc['resource']['descriptions'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "41877433",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We developed an instrument, Critical Engineering Literacy Test (CELT), which is a multiple choice instrument designed to measure undergraduate students’ scientific and information literacy skills. It requires students to first read a technical memo and, based on the memo’s arguments, answer eight multiple choice and six open-ended response questions. We collected data from 143 first-year engineering students and conducted an item analysis. The KR-20 reliability of the instrument was .39. Item difficulties ranged between .17 to .83. The results indicate low reliability index but acceptable levels of item difficulties and item discrimination indices. Students were most challenged when answering items measuring scientific and mathematical literacy (i.e., identifying incorrect information).'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc['resource']['descriptions']['description']['#text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c3d4e9",
   "metadata": {},
   "source": [
    "We can change the text or any other element and reassign it to the json dictionary and save it as xml for uploading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db544fc3",
   "metadata": {},
   "source": [
    "schema validation using the standard schema .xsd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09c5b9c",
   "metadata": {},
   "source": [
    "github.com/inveniosoftware/datacite\n",
    "with JSON to schema converters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e65d7f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
